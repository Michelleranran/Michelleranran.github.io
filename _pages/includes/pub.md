# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/publication/cvpr23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**GFPose: Learning 3D Human Pose Prior with Gradient Fields**

Hai Ci, Mingdong Wu, Wentao Zhu, Xiaoxuan Ma, Hao Dong, **Fangwei Zhong&#x2709;**, Yizhou Wang

**CVPR 2023**,
[Project](https://sites.google.com/view/gfpose/) <strong><span class='show_paper_citations' data='ejDz1bYAAAAJ:8k81kl-MbHgC'></span></strong>, 
[Paper](https://arxiv.org/abs/2212.08641)
[![code](https://img.shields.io/github/stars/Embracing/GFPose?style=social&label=Code+Stars)](https://github.com/Embracing/GFPose) 
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/gfpose-learning-3d-human-pose-prior-with/multi-hypotheses-3d-human-pose-estimation-on)](https://paperswithcode.com/sota/multi-hypotheses-3d-human-pose-estimation-on?p=gfpose-learning-3d-human-pose-prior-with)
- a versatile framework to model plausible 3D human poses for various applications.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2023</div><img src='images/publication/iclr23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Proactive Multi-Camera Collaboration for 3D Human Pose Estimation**

Hai Ci*, Mickel Liu*, Xuehai Pan*, **Fangwei Zhong&#x2709;**, Yizhou Wang

**ICLR 2023**, 
[Project](https://sites.google.com/view/active3dpose) <strong><span class='show_paper_citations' data='ejDz1bYAAAAJ:MXK_kJrjxJIC'></span></strong>,
[Paper](https://openreview.net/pdf?id=CPIy9TWFYBG)
- We propose a novel MARL framework to solve proactive multi-camrea collaborations for 3D HPE in human crowds.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2023</div><img src='images/publication/aaai23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[RSPT: Reconstruct Surroundings and Predict Trajectories for Generalizable Active Object Tracking]()

**Fangwei Zhong***,  Xiao Bi*,  Yudi Zhang,  Wei Zhang, Yizhou Wang

**AAAI 2023** <span style="color:red">(Oral)</span><strong><span class='show_paper_citations' data=''></span></strong>, 
[Project](https://sites.google.com/view/aot-rspt)
- we propose a framework called RSPT to form a structure-aware motion representation by Reconstructing Surroundings and Predicting the target Trajectory.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/publication/neurips22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**TarGF: Learning Target Gradient Field to Rearrange Objects without Explicit Goal Specification**

Mingdong Wu*, **Fangwei Zhong***, Yulong Xia, Hao Dong

[Project](https://sites.google.com/view/targf) <strong><span class='show_paper_citations' data='ejDz1bYAAAAJ:0EnyYjriUFMC'></span></strong>,
[Paper](https://arxiv.org/pdf/2209.00853.pdf),
[Code](https://github.com/AaronAnima/TarGF) 
- We develop a framework based on a target gradient field trained by score-matching to tackle object rearrangement without explicit goal specification.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS D&B 2022</div><img src='images/publication/mate.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**MATE: Benchmarking Multi-Agent Reinforcement Learning in Distributed Target Coverage Control**

Xuehai Pan*, Mickel Liu*, **Fangwei Zhong&#x2709;**, Yaodong Yang&#x2709;, Song-Chun Zhu, Yizhou Wang

[Project](https://github.com/UnrealTracking/mate) <strong><span class='show_paper_citations' data='ejDz1bYAAAAJ:5nxA0vEk-isC'></span></strong>, 
[Paper](https://openreview.net/pdf?id=SyoUVEyzJbE),
[![code](https://img.shields.io/github/stars/UnrealTracking/mate?style=social&label=Code+Stars)](https://github.com/UnrealTracking/mate) 
- A gamification of the multi-camera multi-target target coverage problem, and an all-in-one multi-agent reinforcement learning benchmark
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2022</div><img src='images/publication/icml22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Disentangling Disease-related Representation from Obscure for Disease Prediction**

Chu-Ran Wang, Fei Gao, **Fangwei Zhong&#x2709;**, Fangwei Zhong, Yizhou Yu, Yizhou Wang

[Paper](https://proceedings.mlr.press/v162/wang22f/wang22f.pdf)
- A disentanglement learning strategy under the guidance of alpha blending generation in an encoder-decoder framework (DAB-Net).
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2022</div><img src='images/publication/iclr22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind**

Yuanfei Wang*, **Fangwei Zhong***, Jing Xu, Yizhou Wang

[Paper](hhttps://arxiv.org/pdf/2111.09189.pdf), [Code ![code](https://img.shields.io/github/stars/UnrealTracking/mate?style=social&label=Code+Stars)](https://github.com/UnrealTracking/ToM2C) 
- A Target-oriented Multi-agent Communication and Cooperation mechanism using Theory of Mind. 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2021</div><img src='images/publication/icml21-1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Towards Distraction-Robust Active Visual Tracking**

**Fangwei Zhong**, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang

[Project](https://sites.google.com/view/distraction-robust-avt),
[Paper](http://proceedings.mlr.press/v139/zhong21b/zhong21b.pdf), 
[Code ![code](https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social&label=Code+Stars)](https://github.com/zfw1226/active_tracking_rl/tree/distractor),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- A mixed cooperative-competitive multi-agent game: a target and multiple distractors form a collaborative team to play against a tracker. 
- A bunch of practical methods: a reward function for distractors, a cross-modal teacher-student learning strategy, and a recurrent attention module for the tracker.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2020</div><img src='images/publication/hitmac.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Learning Multi-Agent Coordination for Enhancing Target Coverage in Directional Sensor Networks**

*Jing Xu*, **Fangwei Zhong***, Yizhou Wang

[Project](https://sites.google.com/view/hit-mac),
[Paper](https://proceedings.neurips.cc/paper/2020/file/7250eb93b3c18cc9daa29cf58af7a004-Paper.pdf), 
[Code ![code](https://img.shields.io/github/stars/XuJing1022/HiT-MAC?style=social&label=Code+Stars)](https://github.com/XuJing1022/HiT-MAC),
- a Hierarchical Target-oriented Multi-Agent Coordination (HiT-MAC), which decomposes the target coverage problem into two-level tasks: targets assignment by a coordinator and tracking assigned targets by executors. 
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2020</div><img src='images/publication/aaai20.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Pose-Assisted Multi-Camera Collaboration for Active Object Tracking**

Jing Li*, Jing Xu*, **Fangwei Zhong***, Xiangyu Kong, Yu Qiao, Yizhou Wang

[Project](https://sites.google.com/view/pose-assisted-collaboration),
[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/5419), 
[Code ![code](https://img.shields.io/github/stars/LilJing/pose-assisted-collaboration?style=social&label=Code+Stars)](https://github.com/LilJing/pose-assisted-collaboration),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
[Demo](https://www.youtube.com/watch?v=8Ha7HGkRv6k)
- An efficient yet effective multi-camera collaboration system for collaborative multiCamera active object tracking.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TPAMI</div><img src='images/publication/advatplus.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**AD-VAT+: An Asymmetric Dueling Mechanism for Learning and Understanding Visual Active Tracking**

**Fangwei Zhong**, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang

[Paper](https://ieeexplore.ieee.org/abstract/document/8896000/), 
[Code ![code](https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social&label=Code+Stars)](https://github.com/zfw1226/active_tracking_rl/),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- Employ more advanced environment augmentation technique and two-stage training strategies to improve the performance of the tracker in the case of challenging scenarios such as obstacles.
- Analyze the target‚Äôs behaviors as the training proceeds and visualize the latent space of the tracker for a better understanding.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2019</div><img src='images/publication/advat.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking**

**Fangwei Zhong**, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang

[Paper](https://openreview.net/pdf?id=HkgYmhR9KX), 
[Code ![code](https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social&label=Code+Stars)](https://github.com/zfw1226/active_tracking_rl/),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- A novel adversarial RL method which adopts an Asymmetric Dueling mechanism (tracker vs. target) for robust active visual tracking
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2019</div><img src='images/publication/craves.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**CRAVES: Controlling Robotic Arm with a Vision-based, Economic System**

Yiming Zuo*, Weichao Qiu*, Lingxi Xie, **Fangwei Zhong**, Yizhou Wang, Alan L Yuille

[Project](https://craves.ai/),
[Paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zuo_CRAVES_Controlling_Robotic_Arm_With_a_Vision-Based_Economic_System_CVPR_2019_paper.pdf),
[Code ![code](https://img.shields.io/github/stars/zuoym15/craves.ai?style=social&label=Code+Stars)](https://github.com/zuoym15/craves.ai),
[Controller ![code](https://img.shields.io/github/stars/zfw1226/craves_control?style=social&label=Code+Stars)](https://github.com/zfw1226/craves_control),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- A vision system for low-cost arm control: trains a vision model in virtual environment, and applies it to real-world images after domain adaptation (a semi-supervised approach).
- One virtual environment for collection data and reinforcement learning.
- Two real-world datasets for evaluation.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TPAMI</div><img src='images/publication/craves.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning**

Wenhan Luo*, Peng Sun*, **Fangwei Zhong***, Wei Liu, Tong Zhang, Yizhou Wang

[Paper](https://arxiv.org/pdf/1808.03405.pdf),
[Code ![code](https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social&label=Code+Stars)](https://github.com/zfw1226/active_tracking_rl/),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- First Deploy End-to-end active object tracker trained in virtual environment in real-world robot.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2018</div><img src='images/publication/craves.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**End-to-end Active Object Tracking via Reinforcement Learning**

Wenhan Luo*, Peng Sun*, **Fangwei Zhong**, Wei Liu, Tong Zhang, Yizhou Wang

[Paper](http://proceedings.mlr.press/v80/luo18a/luo18a.pdf),
[Code ![code](https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social&label=Code+Stars)](https://github.com/zfw1226/active_tracking_rl/),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- An end-to-end active objet tracking solution via deep reinforcement learning, where a ConvNet-LSTM function approximator is adopted for the direct frame-to-action prediction.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM</div><img src='images/publication/craves.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Unrealcv: Virtual worlds for computer vision**

Weichao Qiu, **Fangwei Zhong**, Yi Zhang, Siyuan Qiao, Zihao Xiao, Tae Soo Kim, Yizhou Wang, Alan Yuille

[Project](https://unrealcv.org/),
[Paper](https://dl.acm.org/doi/pdf/10.1145/3123266.3129396),
[Code ![code](https://img.shields.io/github/stars/unrealcv/unrealcv?style=social&label=Code+Stars)](https://github.com/unrealcv/unrealcv),
- An open-sourced project to help computer vision researchers build virtual worlds using Unreal Engine 4 (UE4).
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM</div><img src='images/publication/wacv18.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Detect-SLAM: Making Object Detection and SLAM Mutually Beneficial**

**Fangwei Zhong**, Sheng Wang, Ziqi Zhang, Chen Zhou, Yizhou Wang

[Paper](https://ieeexplore.ieee.org/abstract/document/8354219/),
[Video](https://www.youtube.com/watch?v=eqJiyU9ebaY)
- A robotic vision system which integrates SLAM with a deep neural network-based object detector to make the two functions mutually beneficial.
</div>
</div>