# üìù Selected Publications 

\* : co-first author, &#x2709; : corresponding author

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2024</div><img src='images/publication/cvpr23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Fast Peer Adaptation with Context-aware Exploration**

Long Ma, Yuanfei Wang, **Fangwei Zhong&#x2709;**, Song-Chun Zhu, Yizhou Wang

***International Conference on Machine Learning (ICML), 2024***

[Project](https://sites.google.com/view/peer-adaptation), 
[Paper](https://arxiv.org/pdf/2402.02468)
- Learn a context-aware policy with a peer identification reward to effectively explore and quickly adapt to unknown peers. 
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/publication/cvpr23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**GFPose: Learning 3D Human Pose Prior with Gradient Fields**

Hai Ci, Mingdong Wu, Wentao Zhu, Xiaoxuan Ma, Hao Dong, **Fangwei Zhong&#x2709;**, Yizhou Wang

***Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2023***

[Project](https://sites.google.com/view/gfpose/), 
[Paper](https://arxiv.org/abs/2212.08641),
[Code ![code](https://img.shields.io/github/stars/Embracing/GFPose?style=social&label=Code+Stars)](https://github.com/Embracing/GFPose) 
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/gfpose-learning-3d-human-pose-prior-with/multi-hypotheses-3d-human-pose-estimation-on)](https://paperswithcode.com/sota/multi-hypotheses-3d-human-pose-estimation-on?p=gfpose-learning-3d-human-pose-prior-with)
- A versatile framework to model plausible 3D human poses in gradient fields for various applications.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2023</div><img src='images/publication/iclr23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Proactive Multi-Camera Collaboration for 3D Human Pose Estimation**

Hai Ci\*, Mickel Liu\*, Xuehai Pan\*, **Fangwei Zhong&#x2709;**, Yizhou Wang

***International Conference on Learning Representations (ICLR), 2023***

[Project](https://sites.google.com/view/active3dpose),
[Paper](https://openreview.net/pdf?id=CPIy9TWFYBG)
- A novel MARL framework to solve proactive multi-camrea collaborations for 3D HPE in human crowds.
</div>

</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2023</div><img src='images/publication/aaai23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**RSPT: Reconstruct Surroundings and Predict Trajectories for Generalizable Active Object Tracking**

**Fangwei Zhong**\*,  Xiao Bi\*,  Yudi Zhang,  Wei Zhang, Yizhou Wang

***Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI), 2023*** <span style="color:red">(Oral)</span><strong><span class='show_paper_citations' data=''></span></strong>

[Project](https://sites.google.com/view/aot-rspt), [Paper](https://arxiv.org/pdf/2304.03623v1.pdf)
- A framework to form a structure-aware motion representation by Reconstructing Surroundings and Predicting the target Trajectory.
</div>

</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022</div><img src='images/publication/targf_update.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**TarGF: Learning Target Gradient Field to Rearrange Objects without Explicit Goal Specification**

Mingdong Wu\*, **Fangwei Zhong**\*, Yulong Xia, Hao Dong

***Advances in Neural Information Processing Systems (NeurIPS), 2022***

[Project](https://sites.google.com/view/targf),
[Paper](https://arxiv.org/pdf/2209.00853.pdf),
[Code](https://github.com/AaronAnima/TarGF) 
- A framework based on a target gradient field trained by score-matching to tackle object rearrangement without explicit goal specification.
</div>

</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS D&B 2022</div><img src='images/publication/mate.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**MATE: Benchmarking Multi-Agent Reinforcement Learning in Distributed Target Coverage Control**


Xuehai Pan\*, Mickel Liu\*, **Fangwei Zhong&#x2709;**, Yaodong Yang&#x2709;, Song-Chun Zhu, Yizhou Wang

***Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS D&B), 2022***

[Project](https://github.com/UnrealTracking/mate), 
[Paper](https://openreview.net/pdf?id=SyoUVEyzJbE),
[Code ![code](https://img.shields.io/github/stars/UnrealTracking/mate?style=social&label=Code+Stars)](https://github.com/UnrealTracking/mate) 
- A gamification of the multi-camera multi-target target coverage problem, and an all-in-one multi-agent reinforcement learning benchmark
</div>

</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2022</div><img src='images/publication/iclr22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**ToM2C: Target-oriented Multi-agent Communication and Cooperation with Theory of Mind**

Yuanfei Wang\*, **Fangwei Zhong**\*, Jing Xu, Yizhou Wang

***International Conference on Learning Representations (ICLR), 2022***

[Paper](hhttps://arxiv.org/pdf/2111.09189.pdf), [Code ![code](https://img.shields.io/github/stars/UnrealTracking/mate?style=social&label=Code+Stars)](https://github.com/UnrealTracking/ToM2C) 
- A Target-oriented Multi-agent Communication and Cooperation mechanism using Theory of Mind. 
</div>

</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2021</div><img src='images/publication/icml21-1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Towards Distraction-Robust Active Visual Tracking**

**Fangwei Zhong**, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang

***International Conference on Machine Learning (ICML), 2021***

[Project](https://sites.google.com/view/distraction-robust-avt),
[Paper](http://proceedings.mlr.press/v139/zhong21b/zhong21b.pdf), 
[Code ![code](https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social&label=Code+Stars)](https://github.com/zfw1226/active_tracking_rl/tree/distractor),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- A mixed cooperative-competitive multi-agent game: a target and multiple distractors form a collaborative team to play against a tracker. 
- A bunch of practical methods: a reward function for distractors, a cross-modal teacher-student learning strategy, and a recurrent attention module for the tracker.
</div>

</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TPAMI</div><img src='images/publication/advatplus.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**AD-VAT+: An Asymmetric Dueling Mechanism for Learning and Understanding Visual Active Tracking**

**Fangwei Zhong**, Peng Sun, Wenhan Luo, Tingyun Yan, Yizhou Wang

***IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI), 2021***

[Paper](https://ieeexplore.ieee.org/abstract/document/8896000/), 
[Code ![code](https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social&label=Code+Stars)](https://github.com/zfw1226/active_tracking_rl/),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- Employ more advanced environment augmentation technique and two-stage training strategies to improve the performance of the tracker in the case of challenging scenarios such as obstacles.
- Analyze the target‚Äôs behaviors as the training proceeds and visualize the latent space of the tracker for a better understanding.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2019</div><img src='images/publication/craves.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**CRAVES: Controlling Robotic Arm with a Vision-based, Economic System**

Yiming Zuo\*, Weichao Qiu\*, Lingxi Xie, **Fangwei Zhong**, Yizhou Wang, Alan L Yuille

***Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2019***

[Project](https://craves.ai/),
[Paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zuo_CRAVES_Controlling_Robotic_Arm_With_a_Vision-Based_Economic_System_CVPR_2019_paper.pdf),
[Code ![code](https://img.shields.io/github/stars/zuoym15/craves.ai?style=social&label=Code+Stars)](https://github.com/zuoym15/craves.ai),
[Controller ![code](https://img.shields.io/github/stars/zfw1226/craves_control?style=social&label=Code+Stars)](https://github.com/zfw1226/craves_control),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- A vision system for low-cost arm control: trains a vision model in virtual environment, and applies it to real-world images after domain adaptation (a semi-supervised approach).
- One virtual environment for collection data and reinforcement learning.
- Two real-world datasets for evaluation.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TPAMI</div><img src='images/publication/e2e-real.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning**

Wenhan Luo\*, Peng Sun\*, **Fangwei Zhong**\*, Wei Liu, Tong Zhang, Yizhou Wang

***IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE TPAMI), 2020***

[Paper](https://arxiv.org/pdf/1808.03405.pdf),
[Code ![code](https://img.shields.io/github/stars/zfw1226/active_tracking_rl?style=social&label=Code+Stars)](https://github.com/zfw1226/active_tracking_rl/),
[Environment ![code](https://img.shields.io/github/stars/zfw1226/gym-unrealcv?style=social&label=Code+Stars)](https://github.com/zfw1226/gym-unrealcv)
- Deploy End-to-end active object tracker trained in virtual environment in real-world robot.
</div>


</div>
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2017</div><img src='images/publication/unrealcv.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Unrealcv: Virtual worlds for computer vision**

Weichao Qiu, **Fangwei Zhong**, Yi Zhang, Siyuan Qiao, Zihao Xiao, Tae Soo Kim, Yizhou Wang, Alan Yuille

***ACM Multimedia Open Source Software Competition, 2017***

[Project](https://unrealcv.org/),
[Paper](https://dl.acm.org/doi/pdf/10.1145/3123266.3129396),
[Code ![code](https://img.shields.io/github/stars/unrealcv/unrealcv?style=social&label=Code+Stars)](https://github.com/unrealcv/unrealcv)
- An open-sourced project to help computer vision researchers build virtual worlds using Unreal Engine 4 (UE4).
</div>
</div>